import os
import torch
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable

from make_rag_docs import get_rag_data1
from filter_data import *

from langchain_openai import ChatOpenAI
import toml
config = toml.load('.secret/secrets.toml')

api_key = config['OPENAI_API_KEY']


# â­ï¸ Embedding ì„¤ì •
USE_BGE_EMBEDDING = True


# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
# ngrok http --domain=quick-alien-cunning.ngrok-free.app 8000
LANGSERVE_ENDPOINT = "https://quick-alien-cunning.ngrok-free.app/llm/"


# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ê²Œì„ì— ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ Context ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µí•˜ì„¸ìš”. Contextì—ì„œ ì°¸ê³ í–ˆë‹¤ëŠ” ì´ì•¼ê¸°ëŠ” í•˜ì§€ë§ˆì„¸ìš”. ì£¼ì–´ì§„ Contextê°€ ì—†ë‹¤ë©´ 'ì œê°€ ì•Œê³  ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¡œ ë‹µë³€í•˜ì„¸ìš”.
Context: {context} 
Human: {question} 
Assistant:"""


st.set_page_config(page_title="DnF Chatbot", page_icon="ğŸ“”")
st.title("ğŸ“” DnF Chatbot")


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


@st.cache_resource(show_spinner='Get Embedding Model..')
def get_embed_model():

    model_name = "BAAI/bge-m3"
    # GPU Device ì„¤ì •:
    # - NVidia GPU: "cuda"
    # - Mac M1, M2, M3: "mps"
    # - CPU: "cpu"
    model_kwargs = {
        # "device": "cuda"
        "device": "mps"
        # "device": "cpu"
    }
    encode_kwargs = {"normalize_embeddings": True}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
        
    return embeddings


def embed_file(embeddings, **kwargs):
    
    cache_dir = LocalFileStore(f"./.cache/dnf/embedding_3000_withWiki/")
    
    docs = get_rag_data1()

    torch.mps.empty_cache()
    
    bm25_retriever = BM25Retriever.from_documents(
        docs
    )
    bm25_retriever.k = 2
    
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    
    if kwargs:
        retriever = vectorstore.as_retriever(
            # search_type="similarity_score_threshold",
            search_kwargs={
                # 'score_threshold': 0.5,
                # 'score_threshold': 0.2,
                'k' : 10,
                'filter' : kwargs
                }
        )
    else:
        retriever = vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                'score_threshold': 0.6,
                'k' : 2,
                }
        )
        
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, retriever], weights=[0.3, 0.7] # í‚¤ì›Œë“œ ê²€ìƒ‰, ë¬¸ì¥ ê²€ìƒ‰
    )
        
    
    return ensemble_retriever


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    context = ''
    for doc in docs:
        context += doc.page_content
        context += str(doc.metadata)
        context += "\n\n"
    return context[:-2]


embedding = get_embed_model()
print_history()



st.sidebar.write('ê²€ìƒ‰ ì˜ì—­ì„ ì„ íƒí•´ ì£¼ì„¸ìš”. ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.')


search_cate1 = st.sidebar.selectbox("ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", options = SELECT_CATEGORY1)

if search_cate1 == 'ê³µëµ':
    search_cate2 = st.sidebar.selectbox("ê²€ìƒ‰í•  ë˜ì „ì„ ì„ íƒí•˜ì„¸ìš”", options=SELECT_CATEGORY2)


    SELECT_CATEGORY3 = CATEGORY3_DICT[search_cate2]
    
    search_cate3 = st.sidebar.selectbox(f"{search_cate2} ë ˆì´ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", options=SELECT_CATEGORY3)
    
else:
    
    SELECT_CATEGORY3 = CATEGORY3_DICT[search_cate1]
    
    search_cate3 = st.sidebar.selectbox(f"{search_cate1} ê´€ë ¨ ê¶ê¸ˆí•œ ë¶€ë¶„ì„ ì„ íƒí•˜ì„¸ìš”.", options=SELECT_CATEGORY3)
    



if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        llm = RemoteRunnable(LANGSERVE_ENDPOINT)
        # llm = ChatOpenAI(model='gpt-4', openai_api_key = api_key)
        chat_container = st.empty()
        
        print('categories')
        # print(search_cate1, search_cate2, search_cate3)
        # user_input += ' '
        # if search_cate1 : user_input += search_cate1
        # if search_cate2 : user_input += search_cate2
        # if search_cate3 : user_input += search_cate3
        
        with st.spinner("Searching DnF Homepage article..."):
            if search_cate1 != '':
                if search_cate1 == 'ê³µëµ':
                    print('123 kwarg')
                    retriever = embed_file(embedding, 
                                       category1 = search_cate1, 
                                       category2 = search_cate2, 
                                       category3 = search_cate3,
                                       )
                else:
                    print('13 kwarg')
                    retriever = embed_file(embedding, 
                                       category1 = search_cate1, 
                                       category3 = search_cate3, 
                                       )
            else:
                print('None kwarg')
                retriever = embed_file(embedding)
            # retriever = embed_file(embedding)
        
        
        print(user_input)
        # st.write(retriever.invoke(user_input))
        
        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        
        if len(retriever.invoke(user_input)):
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    # "context" : lambda x : format_docs(docs),
                    "context": retriever | format_docs,
                    # 'context' : lambda x : '',
                    "question": RunnablePassthrough(),
                }
                | prompt
                | llm
                | StrOutputParser()
            )
                
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            with st.spinner('ë‹µë³€ ìƒì„± ì¤‘...'):
                answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
                chunks = []
                for chunk in answer:
                    chunks.append(chunk)
                    chat_container.markdown("".join(chunks))
                add_history("ai", "".join(chunks))
        else:
            NONE_PROMPT_TEMPLATE = f"""ë§ì”€í•´ì£¼ì‹  ì§ˆë¬¸ì— ëŒ€í•´ ì œê°€ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."""
            chat_container.markdown(NONE_PROMPT_TEMPLATE)
            add_history("ai", "".join(NONE_PROMPT_TEMPLATE))